{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Corona/Weather Data Science - FYP 2021\n",
    "## How weather influences the spread of the pandemic \n",
    "---\n",
    "### Group 9: Aidan Stocks, Christian Margo Hansen, Jonas-Mika Senghaas, Malthe Pabst, Rasmus Bondo Hansen\n",
    "Submission: 19.03.2021 / Last Modified: 09.03.2021\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains the step-by-step data science process performed on the `IBM Weather Data` from 2020 and the official `Corona Statistics` in 2020. \n",
    "The goal was to inform the authorities of **Germany** (*Group 9 Focus*) about the development of the pandemic in 2020 investigate possible relations between environmental conditions and the spread of the pandemic.\n",
    "\n",
    "The raw datasets were given by the course mananger. *Link source*\n",
    "\n",
    "> *Contact for technical difficulties or questions related to code: Jonas-Mika Senghaas (jsen@itu.dk)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, China. The World Health Organization declared the outbreak a Public Health Emergency of International Concern in January 2020 and a pandemic in March 2020. As of 5 March 2021, more than 115 million cases have been confirmed, with more than 2.56 million deaths attributed to COVID-19, making it one of the deadliest pandemics in history. (Source: [Wikipedia](https://en.wikipedia.org/wiki/COVID-19_pandemic)).\n",
    "\n",
    "The goal of this project is to analyse the development of the pandemic in Germany from February 2020, while paying special attention to environmental factors that may relate to higher chances of infection and therefore a faster spread of the pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this Notebook\n",
    "---\n",
    "This notebook contains all code to reproduce the findings of the project as can be seen on the [GitHub page](https://github.com/jonas-mika/fyp2021p02g09) of this project. In order to read in the data correctly, the global paths configured in the section `Constants` need to be correct. The following file structure - as prepared in the `submission.zip` - was followed throughout the project and is recommended to use (alternatively the paths in the section `Constants` can be adjusted):\n",
    "\n",
    "```\n",
    "submission\n",
    "|   github_link.webloc\n",
    "│   project_report.pdf\n",
    "│   gitlog.txt    \n",
    "│\n",
    "└───data\n",
    "│   │\n",
    "│   | raw\n",
    "│   | │   \n",
    "│   | │   \n",
    "│   | │   \n",
    "|   |\n",
    "|   | \n",
    "|  \n",
    "│   \n",
    "└───notebooks\n",
    "    |   project2.ipynb (current location)\n",
    "    |   project2\n",
    "        |   __init__.py\n",
    "        |   visualisations.py\n",
    "        |   ...\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries\n",
    "---\n",
    "Throughout the project, we will use a range of both built-in and external Python Libraries. This notebook will only run if all libraries and modules are correctly installed on your local machines. \n",
    "To install missing packages use `pip install <package_name>` (PIP (Python Package Index) is the central package management system, read more [here](https://pypi.org/project/pip/)). \n",
    "\n",
    "In case you desire further information about the used packages, click the following links to find detailled documentations:\n",
    "- [Pandas Homepage](https://pandas.pydata.org/)\n",
    "- [Numpy Homepage](https://numpy.org/)\n",
    "- [Matplotlib Homepage](https://matplotlib.org/stable/index.html)\n",
    "- [Folium Documentation](https://python-visualization.github.io/folium/)\n",
    "- [Scipy Homepage](https://www.scipy.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                    # provides major datastructure pd.DataFrame() to store the datasets\n",
    "import numpy as np                                     # used for numerical calculations and fast array manipulations\n",
    "import matplotlib.pyplot as plt                        # visualisation of data\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "import re\n",
    "import folium                                          # spatial visualisation\n",
    "import json                                            # data transfer to json format\n",
    "import os                                              # automates saving of export files (figures, summaries, ...)\n",
    "import random                                          # randomness in coloring of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own Package\n",
    "---\n",
    "Since this project makes heavy use of functions to achieve maximal efficiency, all functions are stored externally in the package structure `project1'. The following imports are necessary for this notebook to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project2.processing import check_columns_for_missing_values\n",
    "from project2.numerical_summary import get_uniques_and_counts, get_fivenumsummary, compute_numerical_summary\n",
    "from project2.visualisations import initialise_summary, barplot, histogram, boxplot, categorical_scatterplot, categorical_association_test\n",
    "from project2.spatial_visualisation import plot_marker, random_color, map_accidents\n",
    "from project2.save import save_csv, save_json, save_figure, save_dict, save_map, save_all_single_variable_analysis, save_all_categorical_scatters, save_all_categorical_associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARK**: All function used in this project are well documented in their `Docstring`. To display the docstring and get an short summary of the function and the specifications of the input argument (including data tupe and small explanation) as well as their return value, type **`?<function_name>`** in Juptyer (*see example*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "---\n",
    "To enhance the readibilty, as well as to decrease the maintenance effort, it is useful for bigger projects to define contants that need to be accessed globally throughout the whole notebook in advance. \n",
    "The following cell contains all of those global constants. By convention, we write them in caps (https://www.python.org/dev/peps/pep-0008/#constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path lookup dictionary to store the relative paths from the directory containing the jupyter notebooks to important directories in the project\n",
    "PATH = {}\n",
    "\n",
    "PATH['data'] = {}\n",
    "PATH['data']['raw'] = \"../data/raw/\"\n",
    "PATH['data']['interim'] = \"../data/interim/\"\n",
    "PATH['data']['processed'] = \"../data/processed/\"\n",
    "PATH['data']['external'] = \"../data/external/\"\n",
    "\n",
    "PATH['corona'] = 'corona/'\n",
    "PATH['metadata'] = 'metadata/'\n",
    "PATH['shapefiles'] = 'shapefiles/'\n",
    "PATH['weather'] = 'weather/'\n",
    "\n",
    "PATH['reports'] = \"../reports/\"\n",
    "\n",
    "# filename lookup dictionary storing the most relevant filenames\n",
    "FILENAME = {}\n",
    "FILENAME['weather'] = 'weather.csv'\n",
    "FILENAME['corona'] = 'de_corona.csv'\n",
    "FILENAME['metadata'] = 'de_metadata.json'\n",
    "FILENAME['shapefiles'] = 'de.geojson'\n",
    "\n",
    "# defining three dictionaries to store data. each dictionary will reference several pandas dataframes\n",
    "DATA_RAW = {}\n",
    "DATA_GERMANY = {}\n",
    "DATA_EXTERNAL = {}\n",
    "\n",
    "REGION_LOOKUP = {}\n",
    "\n",
    "# automising all plots requires a lot of additional information (ie. what plot-type to use on the different variables, whether or not we need a fivenumber-summary, etc.). this information is stored in the summary dictionary\n",
    "SUMMARY = {}\n",
    "    \n",
    "NAMES = {}\n",
    "NAMES['datasets'] = ['weather', 'corona']\n",
    "NAMES['jsons'] = ['metadata', 'shapefiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in metadata using json library\n",
    "for JSON in NAMES['jsons']:\n",
    "    with open(PATH['data']['raw'] + PATH[JSON] + FILENAME[JSON]) as infile:\n",
    "        DATA_GERMANY[JSON] = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_LOOKUP['iso'] = {\n",
    "    DATA_GERMANY['metadata']['country_metadata'][i]['iso3166-2_code']: \n",
    "    DATA_GERMANY['metadata']['country_metadata'][i]['iso3166-2_name_en'] \n",
    "        for i in range(len(DATA_GERMANY['metadata']['country_metadata']))} # key: iso, value: region name\n",
    "\n",
    "REGION_LOOKUP['region'] = {\n",
    "    DATA_GERMANY['metadata']['country_metadata'][i]['iso3166-2_name_en']: \n",
    "    DATA_GERMANY['metadata']['country_metadata'][i]['iso3166-2_code']                            \n",
    "        for i in range(len(DATA_GERMANY['metadata']['country_metadata']))} # key: region name, value: iso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading, Inspection and Processing of Datasets (TASK 0)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Datasets\n",
    "---\n",
    "The data analysis revolves around a handful of datasets from different resources: \n",
    "> *CSV*: Corona (DE) -  Contains the `Number of new infections (per day)` and `Number of new casualties (per day)` filtered by day and region in Germany.\n",
    "\n",
    "> *CSV*: Weather - Contains information about several indicators of weather conditions for each region in Germany, Denmark, Sweden and the Netherlands for each day during the period 13.02.2020 - 21.02.2021 (if `weather.csv` and `weather2.csv` are combined)\n",
    "\n",
    "> *JSON*: Metadata (DE) - Contains more information about the different regions in Germany\n",
    "\n",
    "> *GEOJSON*: Geojson (DE) - Holds the `geojson` data for the different regions in Germany\n",
    "\n",
    "We conveniently load in the `csv` datasets into individual Pandas `DataFrames` using the built-in pandas method `pd.read_csv()` ([Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)). We store those in our `DATA_RAW` dictionary in the corresponding keys.\n",
    "\n",
    "For the `json` and `geojson` files, we use Pythons built-in library `json` in order to store their content in Python `dicts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in weather and corona data using pandas into the predefined dictionary 'DATA_RAW'\n",
    "for dataset in NAMES['datasets']:\n",
    "    DATA_RAW[dataset] = pd.read_csv(PATH['data']['raw'] + PATH[dataset] + FILENAME[dataset], sep = '\\t')"
   ]
  },
  {
   "source": [
    "Unfortunately, the weather data was given in two separate dataframes, where `weather.csv` contains data in the period of 13.02.2020 - 14.11.2020 and `weather2.csv` contains data in the period 15.11.2020 - 21.02.2021.\n",
    "\n",
    "For our analysis we want to consider all weather records in the combined time periods. We therefore read in the `weather2.csv` into another `pd.DataFrame` and then use `pd.concat` to combine our intially loaded weather data with the additional weather data along the vertical axis (`axis=0`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_weather = pd.read_csv(PATH['data']['raw'] + PATH['weather'] + 'weather2.csv', sep= '\\t')\n",
    "DATA_RAW['weather'] = pd.concat([DATA_RAW['weather'], additional_weather])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection of Datasets\n",
    "---\n",
    "We can now have a look at our two dataset to get a good first impression for what kind of data we are dealing with. We start by reporting the number of records and fields/  variables in each of the datasets by using the `shape` property of the Pandas `DataFrame`. \n",
    "We then continue to have an actual look into the data. Similiar to the `head` command in terminal, we can use the method `head()` onto our `DataFrames`, which outputs a nice inline representation of the first five data records of the dataset."
   ]
  },
  {
   "source": [
    "### Time Period"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in NAMES['datasets']:\n",
    "    print(f\"{dataset.capitalize()}: {DATA_RAW[dataset]['date'].iloc[0].replace('-','.')} - {DATA_RAW[dataset]['date'].iloc[-1].replace('-','.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size (Number of Records and Fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in NAMES['datasets']:\n",
    "    print(f\"{dataset.capitalize()}: {DATA_RAW[dataset].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek into Datasets (Describing Attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW['weather'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that the main dataset `weather` stores weather information for the four countries Germany (DE), Denmark (DK), Sweden (SE) and the Netherlands (NL) . It consist of 16.953 rows, corresponding to reports of weather conditions on a specified day in a specified region in one of the countries. The dataset holds the weather data recorded for each of the region from the 13th of February 2020 to the 21st of February 2021. The following variables appear in the dataset:\n",
    "\n",
    "> `DATE` (YYYY-MM-DD): The day of the weather reports (*Time Attribute*)\n",
    "\n",
    "> `ISO 3166-2`: [ISO 2 Code]() for the region, in which the weather report is (*Geographic Attribute*)\n",
    "\n",
    "> `RELATIVE HUMIDITY SURFACE`: (*Numerical Attribute*)\n",
    "\n",
    "> `SOLAR RADIATION`: (*Numerical Attribute*)\n",
    "\n",
    "> `SURFACE PRESSURE`: (*Numerical Attribute*)\n",
    "\n",
    "> `TEMPERATURE ABOVE GROUND`: (*Numerical Attribute*)\n",
    "\n",
    "> `TOTAL PRECIPITATION`: (*Numerical Attribute*)\n",
    "\n",
    "> `UV INDEX`: (*Numerical Attribute*)\n",
    "\n",
    "> `WIND SPEED`: (*Numerical Attribute*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW['corona'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that the dataset `corona` holds information about newly infected cases and new deaths per day per region in Germany for most of the days in 2020. It consist of 5602 rows, corresponding to 5.602 reports of new infections and deaths per day in the different regions of Germany. \n",
    "\n",
    "> `DATE` (YYYY-MM-DD): The day of the reports of new infections and deaths per region\n",
    "\n",
    "> `REGION CODE`: Region in Germany (*Geographic Attribute*)\n",
    "\n",
    "> `CONFIRMED ADDITION`: The number of newly confirmed infections in the region on the specified day (*Numerical Attributes*)\n",
    "\n",
    "> `DECEASED ADDITION`: The number of newly confirmed deaths in the region on the specified day (*Numerical Attributes*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Sanity Check\n",
    "---\n",
    "Before continuing with the data analysis, we want to make sure that the datasets are clean. There are a plentiful of methods to check this. In the following, we will stick to the following three:\n",
    ">(a) **Date Representation** (*Are the dates entered in consistent syntax?*)\n",
    "\n",
    ">(b) **Missing Values in Columns** (*Are there missing values? If yes, how many and in which columns?*)\n",
    "\n",
    ">(c) **Missing Values in Rows** (*Are there days, where there haven't been reports of the weather or corona development?)"
   ]
  },
  {
   "source": [
    "### Date Representation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_date_representation():\n",
    "    for date in DATA_RAW[dataset]['date']:\n",
    "        if not re.match('\\d\\d\\d\\d-\\d\\d-\\d\\d', date):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in NAMES['datasets']:\n",
    "    print(f\"{dataset.capitalize()}: {check_date_representation()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "We check in all columns in all datasets the number of missing values (encoded as `' '` (*empty strings*)) to see if there are missing values (empty string) to get a feeling on which columns we need to further do processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in NAMES['datasets']:\n",
    "    print(dataset.capitalize())\n",
    "    check_columns_for_missing_values(DATA_RAW[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Datasets \n",
    "---\n",
    "We got a good first feeling for both datasets and have proven them to be clean. The next step is to filter both datasets to only hold data related to Germany. Our goal is to not only have a dataframe that holds the `weather` and `corona` data for the whole of Germany, but also for each of the 16 regions in Germany.\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Entire Germany\n",
    "We therefore start by defining another empty dictionary in `DATA_GERMANY['corona']` and `DATA_GERMANY['weather']`. These will hold the `corona` and `weather` data for the whole of Germany at key `all` and each region's at the corresponding `region_name` as a key."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in NAMES['datasets']:\n",
    "    DATA_GERMANY[dataset] = {}"
   ]
  },
  {
   "source": [
    "The `corona` and `weather` data for entire Germany is easy to obtain. Since the `corona` dataset is already filtered for Germany, we only need to copy the raw corona data into the key `all`.\n",
    "The `weather` data, however, contains information for four countries, namely Denmark, Sweden, Netherlands and Germany. Here we need to filter. We do this by the unique region_codes that are used to map the weather data to the different secondary-class regions of each country. We can obtain these codes from the metadata provided for Germany that is saved at `DATA_RAW['metadata']`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "region_codes = [DATA_GERMANY['metadata']['country_metadata'][i]['iso3166-2_code'] \n",
    "                for i in range(len(DATA_GERMANY['metadata']['country_metadata']))]\n",
    "\n",
    "DATA_GERMANY['corona']['all'] = DATA_RAW['corona']\n",
    "DATA_GERMANY['weather']['all'] = DATA_RAW['weather'][DATA_RAW['weather']['iso3166-2'].isin(region_codes)]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Regional Filtering\n",
    "We now aim to filter the obtained datasets that hold the `corona` and `weather` data for all of Germany for each of the 16 regions and save them at the corresponding key in `DATA_GERMANY`. We do this by iterating over the region codes (ISO 3166-2 Standard) that we obtained from the metadata. \n",
    "\n",
    "In the `weather` dataset, we can use these iso-values to use a simple mask onto the german weather data in order to filter for each region. \n",
    "In the `corona` dataset, however, the regions are not defined by their iso-value, but their regular name. In order to filter correctly, we use our `REGION_LOOKUP` mapping that for an iso-value as key, which returns the regular region name. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_codes:\n",
    "    mask = DATA_GERMANY['weather']['all']['iso3166-2'] == region\n",
    "    DATA_GERMANY['weather'][REGION_LOOKUP['iso'][region].lower()] = DATA_GERMANY['weather']['all'][mask].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_codes:\n",
    "    mask = DATA_GERMANY['corona']['all']['region_code'] == REGION_LOOKUP['iso'][region]\n",
    "    DATA_GERMANY['corona'][REGION_LOOKUP['iso'][region].lower()] = DATA_GERMANY['corona']['all'][mask].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection\n",
    "---\n",
    "With our newly filtered datasets, we have reduced the number of `weather` and `corona` data both to the whole of Germany and to each of its regions, and we can convienently access them by looking them up in `DATA_GERMANY[dataset]` using the region name as a key. To see, with what kind of data we are dealing now, it makes to recompute the size of each of the datasets. However, our analysis of the column attributes still holds, since we have only filtered across `axis=0` (the rows), while we left the attributes untouched. \n",
    "\n",
    "Again, we report the number of records and fields/ variables in each of the datasets by using the `shape` property of the Pandas `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in NAMES['datasets']:\n",
    "    print(f\"{'-'*5}{dataset.capitalize()}{'-'*5}\")\n",
    "    for key in DATA_GERMANY[dataset].keys():\n",
    "        print(f\"{key.title()}: {DATA_GERMANY[dataset][key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Filtered Data\n",
    "---\n",
    "We can now saave our filtered data to the `../data/interim/`, which saves our filtered data into `csv` format for later inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in NAMES['datasets']:\n",
    "    for key in DATA_GERMANY[dataset].keys():\n",
    "        save_csv(DATA_GERMANY[dataset][key], path=PATH['data']['interim'] + PATH[dataset], filename=f\"{dataset}_{key}\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "---\n",
    "As our sanity checks have proven, the provided data is already quite clean. In all datasets, there are neither values missing nor inconcistencies in the recording of data (ie. representation of dates). \n",
    "\n",
    "However, to make our futher analysis more pleasant, there are some minor changes that we will perform on our datasets:\n",
    "> (a) **Renaming of Columns** (The naming of the columns in the original dataframes is inconcistent and - in parts - poorly descriptive (What are ie. `Confirmed Addition`?))\n",
    "\n",
    "> (b) **Relative Data** (In our later analysis, we do not only want to look at absolute values, but want to interpret those in relation to the size of the region we are looking at)\n",
    "\n",
    "> (c) **Metric Change** (Here we change the `Temperature` that was recorded in Kelvin into Celcius for easier readability)"
   ]
  },
  {
   "source": [
    "### a. Renaming of Columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in DATA_GERMANY['weather'].keys():\n",
    "    DATA_GERMANY['weather'][dataframe].rename(columns={\n",
    "        'date': 'date', \n",
    "        'iso3166-2': 'iso3166-2',\n",
    "        'RelativeHumiditySurface': 'relative_humidity_surface', \n",
    "        'SolarRadiation': 'solar_radiation', \n",
    "        'Surfacepressure': 'surface_pressure', \n",
    "        'TemperatureAboveGround': 'temperature_above_ground', \n",
    "        'Totalprecipitation': 'total_precipiation', \n",
    "        'UVIndex': 'uv_index', \n",
    "        'WindSpeed': 'wind_speed'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in DATA_GERMANY['corona'].keys():\n",
    "    DATA_GERMANY['corona'][dataframe].rename(columns={\n",
    "        'date': 'date', \n",
    "        'region_code': 'region', \n",
    "        'confirmed_addition': 'absolute_infections', \n",
    "        'deceased_addition': 'absolute_deaths'}, inplace=True)"
   ]
  },
  {
   "source": [
    "### b. Relative Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_map = {DATA_GERMANY['metadata']['country_metadata'][i]['iso3166-2_name_en']:\n",
    "                  DATA_GERMANY['metadata']['country_metadata'][i]['population'] \n",
    "                  for i in range(len(DATA_GERMANY['metadata'][\"country_metadata\"]))\n",
    "                 }\n",
    "for region in DATA_GERMANY['corona'].keys():\n",
    "    DATA_GERMANY['corona'][region]['population'] = DATA_GERMANY['corona'][region]['region'].map(population_map)\n",
    "    DATA_GERMANY['corona'][region]['relative_infections'] = DATA_GERMANY['corona'][region]['absolute_infections']/DATA_GERMANY['corona'][region]['population']*100\n",
    "    DATA_GERMANY['corona'][region]['relative_deaths'] = DATA_GERMANY['corona'][region]['absolute_deaths']/DATA_GERMANY['corona'][region]['population']*100"
   ]
  },
  {
   "source": [
    "### c. Metric Change"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in DATA_GERMANY['weather'].keys():\n",
    "    DATA_GERMANY['weather'][region]['temperature_above_ground'] = DATA_GERMANY['weather'][region]['temperature_above_ground'] - 273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Processed Datasets\n",
    "--- \n",
    "Finally, we export the processed datasets into new subfolders in `../data/processed`. From now on, the Jupyter will sorely work with these processed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in NAMES['datasets']:\n",
    "    for key in DATA_GERMANY[dataset].keys():\n",
    "        save_csv(DATA_GERMANY[dataset][key], path=PATH['data']['processed'] + PATH[dataset], filename=f\"{dataset}_{key}\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Variable Analysis (TASK 1)\n",
    "---\n",
    "We have obtained filtered and processed datasets from TASK 0. We will now turn our focus to analysing each attribute in each of the datasets both in a numerical summary (ie. through the number of uniques, counts of uniques or a five-number summary, where it makes sense) and visual representation. Depending on the type of the variable, there arouse different preferred ways of visual repsentation, to get a visual feeling for the data we are dealing with. \n",
    "> `Barplot` (*Categorical Variables*)\n",
    ">> We usually want to plot categorical variables in a bar plot, where the x-axis is labelled with the unique values measured in the specific column and the y-axis represents the number of occurences of each unique value. An example of a bar plot in this project is ie. the Number of Accidents per Week Day.\n",
    "\n",
    "> `Histogram` (*Numerical Variables*)\n",
    ">> Most numerical variables (especially those with a wide range), are plotted in a histogram (which accounts for the fact that numerical variables are continuous). An example of a histogram is ie. the Age Distribution of Casualties involved in an Accident.\n",
    "\n",
    "> `None` (*Others*)\n",
    ">> For some variables it doesn't make sense to plot them, ie. geographical (`Longitude`, ...) or relatinonal attributes (`Accident_Index`, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Analysis\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    compute_numerical_summary(SUMMARY[dataset], DATA_LEEDS[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Numerical Reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "---\n",
    "An essential step of each data exploration is to visualise our data. Only this makes the abstract numbers and characters meaningful.\n",
    "\n",
    "We are therefore visualising the results of the single variable analysis in this section using the data structure `SUMMARY` we have step-by-step developed throughout the project. Depending on the properties of the attribute to plot, there arise three types of visualisatin:\n",
    "> `Barplot`: The standard way of representing `categorical variables` using rectangular bars with heights and widths ([More information](https://en.wikipedia.org/wiki/Bar_chart))\n",
    "\n",
    "> `Histogram`: A histogram is an approximate representation of the distribution of numerical data ([More information](https://en.wikipedia.org/wiki/Histogram))\n",
    "\n",
    "> `Boxplot`: Boxplots are an important statistical tool to represent five-number summaries of numerical data visually ([More information](https://en.wikipedia.org/wiki/Box_plot))\n",
    "\n",
    "To efficiently plot everything, we defined three unique functions `barplot`, `histogram` and `boxplot` that all take in the `SUMMARY` data structure as their main argument and 'intelligently' combine all informatation we have previously gathered to give a nice visualisation of the data.\n",
    "We use the function `save_all_single_variable_analysis` that depends on `save_figure` to iterate over all columns and for each attributes computes and saves the correct plot in the correct directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(DATA_GERMANY['weather']['hamburg']['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weather(data, region, condition, aggregation='daily', dimensions=(16,9)):\n",
    "    fig = plt.figure(figsize=dimensions)\n",
    "    ax = fig.add_axes([.15,.15,.7,.7]) # [left, bottom, width, height]\n",
    "\n",
    "    region_data = data[region]\n",
    "    region_data['date'] = pd.to_datetime(region_data['date'], format='%Y-%m-%d')\n",
    "\n",
    "    # data to plot for each level of aggregation\n",
    "    if region == 'all':\n",
    "        region_data = pd.DataFrame(region_data.groupby('date')[condition].mean()).reset_index()\n",
    "\n",
    "    if aggregation == 'daily':\n",
    "        to_plot = region_data[['date', condition]]\n",
    "    elif aggregation == 'weekly':\n",
    "        to_plot = pd.DataFrame(region_data.groupby(pd.Grouper(key='date', freq='W-MON'))[condition].mean()).reset_index()\n",
    "    elif aggregation == 'monthly':\n",
    "        to_plot = pd.DataFrame(region_data.groupby(pd.Grouper(key='date', freq='M'))[condition].mean()).reset_index()\n",
    "\n",
    "    if region == 'all': title = f\"{condition.title().replace('_',' ')} in Germany\"        \n",
    "    else: title = f\"{condition.title().replace('_',' ')} in {region.title()}\"\n",
    "\n",
    "    # plot\n",
    "    ax.plot(to_plot['date'], to_plot[condition], '-o', color='darkblue', linewidth='2')\n",
    "\n",
    "    # labeling\n",
    "    ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(f\"{condition.title().replace('_', ' ')}\")\n",
    "\n",
    "    # labelling of x-axis (https://stackoverflow.com/questions/9627686/plotting-dates-on-the-x-axis-with-pythons-matplotlib)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%Y'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    ax.set_ylim(min(data['all'][condition]), max(data['all'][condition]))\n",
    "\n",
    "    ax.text(.76,.89, f\"{aggregation.capitalize()} Records\", verticalalignment='center', transform = ax.transAxes, fontweight='bold')\n",
    "    ax.text(.76,.84, f\"Period: Feb 2020-Feb 2021\", verticalalignment='center', transform = ax.transAxes)\n",
    "\n",
    "    # activate grid\n",
    "    ax.grid(True)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plot_weather(\n",
    "    data = DATA_GERMANY['weather'],\n",
    "    region = 'hamburg',\n",
    "    condition = 'temperature_above_ground',\n",
    "    aggregation= 'daily'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weather_all(data, condition, dimensions=(16,9)):\n",
    "    fig = plt.figure(figsize=dimensions)\n",
    "    ax = fig.add_axes([.15,.15,.7,.7]) # [left, bottom, width, height]\n",
    "\n",
    "    for region in list(DATA_GERMANY['weather'].keys())[1:]:\n",
    "        x = pd.to_datetime(data[region]['date'], format='%Y-%m-%d')\n",
    "        y = data[region][condition]\n",
    "        # plot\n",
    "        ax.plot(x,y, label=region.title())\n",
    "\n",
    "    # labeling\n",
    "    ax.set_title(f\"{condition.title().replace('_', ' ')} in all Regions (Feb 2020-Feb 2021)\", fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel(\"date\")\n",
    "    ax.set_ylabel(f\"{condition.title().replace('_', ' ')}\")\n",
    "\n",
    "    # labelling of x-axis (https://stackoverflow.com/questions/9627686/plotting-dates-on-the-x-axis-with-pythons-matplotlib)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%Y'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    ax.set_ylim(min(data['all'][condition]), max(data['all'][condition]))\n",
    "\n",
    "    # activate grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_weather_all(\n",
    "    data = DATA_GERMANY['weather'],\n",
    "    condition = 'uv_index'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_covid_cases(data, region, focus='infections', scale='absolute', aggregation='daily', dimensions=(16,9)):\n",
    "    fig = plt.figure(figsize=dimensions)\n",
    "    ax = fig.add_axes([.15,.15,.7,.7]) # [left, bottom, width, height]\n",
    "\n",
    "    condition = f\"{scale}_{focus}\"\n",
    "\n",
    "    region_data = data[region]\n",
    "    region_data['date'] = pd.to_datetime(region_data['date'], format='%Y-%m-%d')\n",
    "\n",
    "    # data to plot for each level of aggregation\n",
    "    if region == 'all':\n",
    "        region_data = pd.DataFrame(region_data.groupby('date')[condition].mean()).reset_index()\n",
    "\n",
    "    if aggregation == 'daily':\n",
    "        to_plot = region_data[['date', condition]]\n",
    "    elif aggregation == 'weekly':\n",
    "        to_plot = pd.DataFrame(region_data.groupby(pd.Grouper(key='date', freq='W-MON'))[condition].mean()).reset_index()\n",
    "    elif aggregation == 'monthly':\n",
    "        to_plot = pd.DataFrame(region_data.groupby(pd.Grouper(key='date', freq='M'))[condition].mean()).reset_index()\n",
    "\n",
    "    if region == 'all': title = f\"{scale.title()} Number of {focus.title()} in Germany\"        \n",
    "    else: \n",
    "        title = f\"{scale.title()} Number of {focus.title()} in {region.title()}\"\n",
    "        ax.set_ylim(0, max(data['all'][condition]))\n",
    "                    \n",
    "    # plot\n",
    "    ax.plot(to_plot['date'], to_plot[condition], '-o', color='darkblue', linewidth=2)\n",
    "\n",
    "    # labeling\n",
    "    ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(f\"{scale.capitalize()} Number of {focus.capitalize()}\")\n",
    "\n",
    "    # labelling of x-axis (https://stackoverflow.com/questions/9627686/plotting-dates-on-the-x-axis-with-pythons-matplotlib)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%Y'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    # activate grid\n",
    "    ax.grid(True)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_covid_cases(data = DATA_GERMANY['corona'], region='hamburg', focus='infections', scale='absolute', aggregation='weekly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Figures\n",
    "---\n",
    "...\n",
    "\n",
    "**REMARK**: Be aware that the cell takes some time to compute, since it plots and saves all figures at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for region in DATA_GERMANY['weather'].keys():\n",
    "    for condition in list(DATA_GERMANY['weather']['all'])[2:]:\n",
    "        save_figure(figure = plot_weather(data = DATA_GERMANY['weather'], region = region, condition = condition),\n",
    "                    path = PATH['reports'] + f\"{region}/weather\", filename = f\"{condition.lower().replace(' ', '_')}\", save_to = 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for region in DATA_GERMANY['weather'].keys():\n",
    "    for scale in ['absolute', 'relative']:\n",
    "        save_figure(figure = plot_covid_cases(data = DATA_GERMANY['corona'],region = region,condition = 'Confirmed Addition',scale = scale), path = PATH['reports'] + f\"{region}/infections\", filename = f\"{scale}_infections_per_day\", save_to = 'pdf')\n",
    "        \n",
    "        save_figure(figure = plot_covid_cases(data = DATA_GERMANY['corona'],region = region,condition = 'Deceased Addition',scale = scale), path = PATH['reports'] + f\"{region}/deceased\", filename = f\"{scale}_deceased_per_day\", save_to = 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports of Single Variable Analysis\n",
    "---\n",
    "`TASK 1` specifically asks us to report []...\n",
    "> a. ...\n",
    "\n",
    "> b. ...\n",
    "\n",
    "The following reports (that can also be found in the automatically generated plots in `reports/`) answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associations (TASK 2)\n",
    "---\n",
    "An equally important, and perhaps more challenging, part of any data analysis is to find associations in a dataset. Especially when analysing *Road Safety* this is a key aspect of the analysis, as we can investigate which accident, casualty or vehicle attribute lead to more frequent or severe accidents. These insights, in turn, allow us to proactively counteract through political measures. \n",
    "\n",
    "Since our dataset is dominated by `categorical variables` (as seen in the `Initial Inspection of Datasets`), we mostly want to relate `categorical variables` to each other and the few `numerical variables` that exist there. The following methods of associating are used in the following section to investigate these associations:\n",
    "> **`Categorical/ Numerical`**\n",
    ">> For relating a categorical to a numerical variable we use so-called `categorical scatterplots`, which are a special kind of scatterplot which displays the distribution of a numerical variable for different attributes in the cateogrical variable. To plot these kind of plots we use our own function `categorical_scatterplot` that at its core depends on `sns.catplot` ([Documentation](https://seaborn.pydata.org/generated/seaborn.catplot.html))\n",
    "\n",
    "> **`Categorical/ Categorical`**\n",
    ">> Relating two categorical attributes to one another is slightly more difficult. To do this, we count the number of occurrences in all possible combinations of the two categorical variables and plot those in individual plots. To investigate the associativity between the two categorical variables, we use the [Pearson Chi Squared](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test) test. It is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. It follows the general assumption, that we expect no change in the relative observed values if there is no association at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking Focus\n",
    "---\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assocations\n",
    "---\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving all Associations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report: Association between `Accident` and `Vehicle/ Casualty` Attribute \n",
    "---\n",
    "`TASK 2` ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Visualisation (TASK 3)\n",
    "---\n",
    "So far, we have completely ignored the `geographic attributes` that were recorded for each accident in `accidents`. We will change this now by plotting Leed's accidents on a map to get a good visual intuition on where the accidents happen. To do this, we use a combination of function, namely `map_accidents` that under the hood calls `plot_point`. The functions are based on `folium` - an external Python package - that was earlier introduced and makes use of leaflet.js, which is a JavaScript package used to create interactive maps. \n",
    "After export, can be explored as an `html` file in the browser or inline within Jupyter. The map will contain a heat_map (if function argument `heat_map` = `True`), and for an arbitary of column in `accidens` as an argument for `focus` will make a visual distinctions for different values of this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all Accidents in Leeds onto Map + Save Map Visualisation\n",
    "---\n",
    "Through the below cell, we save all maps generated with focuses on different attributes into the directory `../reports/leeds/maps`. We can study them in further detail by opening the `html` files in the browser.\n",
    "\n",
    "**REMARK**: Note that the execution of the below cell may take some time, since two maps (each ~3MB) need to be rendered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corona_region = pd.DataFrame()\n",
    "labels = np.array(list(DATA_GERMANY['corona']['all']))[[2,3,5,6]]\n",
    "for region in list(DATA_GERMANY['corona'].keys())[1:]:\n",
    "    row = DATA_GERMANY['corona'][region].sum().drop(['date', 'population','region'])\n",
    "    _dict = {labels[i]:row.iloc[i] for i in range(len(row))}\n",
    "    _dict['region_code'] = REGION_LOOKUP['region'][region.title()]\n",
    "    data_corona_region = data_corona_region.append(pd.Series(_dict, name=region.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_GERMANY['corona']['hamburg'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather_region = pd.DataFrame()\n",
    "labels = list(DATA_GERMANY['weather']['all'])[2:]\n",
    "for region in list(DATA_GERMANY['weather'].keys())[1:]:\n",
    "    row = DATA_GERMANY['weather'][region].mean()\n",
    "    _dict = {labels[i]: row.iloc[i] for i in range(len(row))}\n",
    "    _dict['region_code'] = REGION_LOOKUP['region'][region.title()]\n",
    "    data_weather_region = data_weather_region.append(pd.Series(_dict, name=region.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chloropleth(data, condition):\n",
    "    #create the folium.Choropleth\n",
    "    m = folium.Map(location = [51.3, 10.3], zoom_start = 6,)\n",
    "\n",
    "    # plot onto chloropleth\n",
    "    folium.Choropleth(\n",
    "        geo_data = DATA_GERMANY['shapefiles'],\n",
    "        data = data,\n",
    "        columns = [\"region_code\", condition],\n",
    "        key_on = \"properties.iso_3166_2\",\n",
    "        fill_color = \"OrRd\",\n",
    "        fill_opacity = .6,\n",
    "        line_opacity = .5,\n",
    "        legend_name = f\"{condition.title().replace('_', ' ')} (Time Period: February 2020 - February 2021)\",\n",
    "    ).add_to(m)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chloropleth(data=data_weather_region, condition='wind_speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chloropleth(data=data_corona_region, condition='relative_infections')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report: Relevant Map\n",
    "---\n",
    "TASK 2 specifically asks us to visualise  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reported map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Question (TASK 4)\n",
    "---\n",
    "The final `TASK 4` asks us to investigate a self-chosen research question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "---\n",
    "Relevant Literature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}